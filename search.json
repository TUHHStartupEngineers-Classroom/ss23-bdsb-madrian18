[
  {
    "objectID": "content/03_other/06_links.html",
    "href": "content/03_other/06_links.html",
    "title": "Links",
    "section": "",
    "text": "R is a free open-source programming language that can be used for statistical analysis, data-simulation, graphing, and lots of other stuff. Another free program is R-studio, that provides a nice graphic interface for R. Download R first, then download R-studio. Both can run on PCs, Macs or Linux. Students will be learning R in the stats labs using the lab manual .\n\n\n\n\nGoogle is great, Google your problem\nStackoverflow is great, google will often take you there because someone has already asked your question, and someone else has answered, usually many people have answered your question many ways."
  },
  {
    "objectID": "content/01_journal/03_data_wrangling.html",
    "href": "content/01_journal/03_data_wrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "In this section, the important process of data wrangling is acquired in detail. Therefore, data can further be prepared for the analysis and the foundation of exploratory data analysis. Moreover, the concept of data.table for handling big data is introduced too."
  },
  {
    "objectID": "content/01_journal/03_data_wrangling.html#source-code-result",
    "href": "content/01_journal/03_data_wrangling.html#source-code-result",
    "title": "Data Wrangling",
    "section": "\n2.1 Source Code & Result",
    "text": "2.1 Source Code & Result\n\n# 1.0 LOAD LIBRARY ----\n\n# Tidyverse\nlibrary(tidyverse)\n\n#> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.2     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#> ✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n#> ✔ purrr     1.0.1     \n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(vroom)\n\n# Data Table\nlibrary(data.table)\n\n#> \n#> Attaching package: 'data.table'\n#> \n#> The following objects are masked from 'package:lubridate':\n#> \n#>     hour, isoweek, mday, minute, month, quarter, second, wday, week,\n#>     yday, year\n#> \n#> The following objects are masked from 'package:dplyr':\n#> \n#>     between, first, last\n#> \n#> The following object is masked from 'package:purrr':\n#> \n#>     transpose\n\n# 2.0 DATA IMPORT ----\n\n# 2.1 Assignee Data ----\n\ncol_types_assigne <- list(\n  id = col_character(),\n  type = col_integer(),\n  organization = col_character()\n)\n\nassignee_data <- vroom(\n  file       = \"../../Patent_data_reduced/assignee.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types_assigne,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\n\n# 2.2 Patent Assignee Data ----\n\ncol_types_patent_assignee <- list(\n  patent_id = col_character(),\n  assignee_id = col_character()\n)\n\npatent_assignee_data <- vroom(\n  file       = \"../../Patent_data_reduced/patent_assignee.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types_patent_assignee,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\n\n# 2.3 Patent Data ----\n\ncol_types_patent <- list(\n  id = col_character(),\n  date = col_date(\"%Y-%m-%d\"),\n  num_claims = col_integer()\n)\n\npatent_data <- vroom(\n  file       = \"../../Patent_data_reduced/patent.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types_patent,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\n\n# 2.4 USPC Data ----\n\ncol_types_uspc <- list(\n  patent_id = col_character(),\n  mainclass_id = col_character(),\n  sequence = col_integer()\n)\n\nuspc_data <- vroom(\n  file       = \"../../Patent_data_reduced/uspc.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types_uspc,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\n\n# 3.0 CONVERT TO DATA.TABLE ----\n\n# 3.1 Assignee Data ----\n\nsetDT(assignee_data)\n\n# 3.2 Patent Assignee Data ----\n\nsetDT(patent_assignee_data)\n\n# 3.3 Patent Data ----\n\nsetDT(patent_data)\n\n# 3.2 USPC Data ----\n\nsetDT(uspc_data)\n\n# 4.0 DATA WRANGLING ----\n\n# 4.1 Joining / Merging Data ----\n\n# for question 1\ncombined1_data <- merge(x = patent_assignee_data, y = assignee_data, \n                       by.x = \"assignee_id\", by.y = \"id\", \n                       all.x = TRUE, \n                       all.y = FALSE)\n\nsetkey(combined1_data, \"type\")\nkey(combined1_data)\n\n#> [1] \"type\"\n\nsetorderv(combined1_data, c(\"type\"))\n\n\n# 4.4 Grouped Mutations ----\n\n# for question 1\n\n# 4.4.1.1 find the company from type = 2 with the most patent\ncombined1_data[type == 2, .(count = .N), by = organization][order(-count)][1]\n\n\n\n  \n\n\n# 4.4.1.2 find the top 10 companies from type = 2 with the most patent\ncombined1_data[type == 2, .(count = .N), by = organization][order(-count)][1:10]"
  },
  {
    "objectID": "content/01_journal/03_data_wrangling.html#source-code-result-1",
    "href": "content/01_journal/03_data_wrangling.html#source-code-result-1",
    "title": "Data Wrangling",
    "section": "\n3.1 Source Code & Result",
    "text": "3.1 Source Code & Result\n\n# 1.0 LOAD LIBRARY ----\n\n# Tidyverse\nlibrary(tidyverse)\nlibrary(vroom)\n\n# Data Table\nlibrary(data.table)\n\n# 2.0 DATA IMPORT ----\n\n# 2.1 Assignee Data ----\n\ncol_types_assigne <- list(\n  id = col_character(),\n  type = col_integer(),\n  organization = col_character()\n)\n\nassignee_data <- vroom(\n  file       = \"../../Patent_data_reduced/assignee.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types_assigne,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\n\n# 2.2 Patent Assignee Data ----\n\ncol_types_patent_assignee <- list(\n  patent_id = col_character(),\n  assignee_id = col_character()\n)\n\npatent_assignee_data <- vroom(\n  file       = \"../../Patent_data_reduced/patent_assignee.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types_patent_assignee,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\n\n# 2.3 Patent Data ----\n\ncol_types_patent <- list(\n  id = col_character(),\n  date = col_date(\"%Y-%m-%d\"),\n  num_claims = col_integer()\n)\n\npatent_data <- vroom(\n  file       = \"../../Patent_data_reduced/patent.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types_patent,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\n\n# 2.4 USPC Data ----\n\ncol_types_uspc <- list(\n  patent_id = col_character(),\n  mainclass_id = col_character(),\n  sequence = col_integer()\n)\n\nuspc_data <- vroom(\n  file       = \"../../Patent_data_reduced/uspc.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types_uspc,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\n\n# 3.0 CONVERT TO DATA.TABLE ----\n\n# 3.1 Assignee Data ----\n\nsetDT(assignee_data)\n\n# 3.2 Patent Assignee Data ----\n\nsetDT(patent_assignee_data)\n\n# 3.3 Patent Data ----\n\nsetDT(patent_data)\n\n# 3.2 USPC Data ----\n\nsetDT(uspc_data)\n\n# 4.0 DATA WRANGLING ----\n\n# 4.1 Joining / Merging Data ----\n\n# for question 2\n\ncombined2_data <- merge(x = combined1_data, y = patent_data, \n                        by.x = \"patent_id\", by.y = \"id\", \n                        all.x = TRUE, \n                        all.y = FALSE)\n\nsetkey(combined2_data, \"type\")\nkey(combined2_data)\n\n#> [1] \"type\"\n\nsetorderv(combined2_data, c(\"type\"))\n\n\n# 4.4 Grouped Mutations ----\n\n# for question 2\n\n# 4.4.2.1 find the company from type = 2 with the most patent on Aug-2014\ncombined2_data[type == 2 & lubridate::month(date, label = T, abbr = F) == \"August\" & lubridate::year(date) == \"2014\", .(count = .N), by = organization][order(-count)][1]\n\n\n\n  \n\n\n# 4.4.2.2 find the top 10 companies from type = 2 with the most patent on Aug-2014\ncombined2_data[type == 2 & lubridate::month(date, label = T, abbr = F) == \"August\" & lubridate::year(date) == \"2014\", .(count = .N), by = organization][order(-count)][1:10]"
  },
  {
    "objectID": "content/01_journal/03_data_wrangling.html#source-code-result-2",
    "href": "content/01_journal/03_data_wrangling.html#source-code-result-2",
    "title": "Data Wrangling",
    "section": "\n4.1 Source Code & Result",
    "text": "4.1 Source Code & Result\n\n# 1.0 LOAD LIBRARY ----\n\n# Tidyverse\nlibrary(tidyverse)\nlibrary(vroom)\n\n# Data Table\nlibrary(data.table)\n\n# 2.0 DATA IMPORT ----\n\n# 2.1 Assignee Data ----\n\ncol_types_assigne <- list(\n  id = col_character(),\n  type = col_integer(),\n  organization = col_character()\n)\n\nassignee_data <- vroom(\n  file       = \"../../Patent_data_reduced/assignee.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types_assigne,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\n\n# 2.2 Patent Assignee Data ----\n\ncol_types_patent_assignee <- list(\n  patent_id = col_character(),\n  assignee_id = col_character()\n)\n\npatent_assignee_data <- vroom(\n  file       = \"../../Patent_data_reduced/patent_assignee.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types_patent_assignee,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\n\n# 2.3 Patent Data ----\n\ncol_types_patent <- list(\n  id = col_character(),\n  date = col_date(\"%Y-%m-%d\"),\n  num_claims = col_integer()\n)\n\npatent_data <- vroom(\n  file       = \"../../Patent_data_reduced/patent.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types_patent,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\n\n# 2.4 USPC Data ----\n\ncol_types_uspc <- list(\n  patent_id = col_character(),\n  mainclass_id = col_character(),\n  sequence = col_integer()\n)\n\nuspc_data <- vroom(\n  file       = \"../../Patent_data_reduced/uspc.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types_uspc,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\n\n# 3.0 CONVERT TO DATA.TABLE ----\n\n# 3.1 Assignee Data ----\n\nsetDT(assignee_data)\n\n# 3.2 Patent Assignee Data ----\n\nsetDT(patent_assignee_data)\n\n# 3.3 Patent Data ----\n\nsetDT(patent_data)\n\n# 3.2 USPC Data ----\n\nsetDT(uspc_data)\n\n# 4.0 DATA WRANGLING ----\n\n# 4.1 Joining / Merging Data ----\n\n# for question 3\n\nuspc1_data <- uspc_data[, lapply(.SD, mean), \n          by = .(patent_id, mainclass_id), \n          .SDcols = c(\"sequence\")]\n\ncombined3_data <- merge(x = combined2_data, y = uspc1_data, \n                        by = \"patent_id\", \n                        all.x = TRUE, \n                        all.y = FALSE)\n\nsetkey(combined3_data, \"type\")\nkey(combined3_data)\n\n#> [1] \"type\"\n\nsetorderv(combined3_data, c(\"type\"))\n\n# 4.4 Grouped Mutations ----\n\n# for question 3\n\n# 4.4.3.1 most innovative tech sector\ncombined3_data[!is.na(mainclass_id), .(count = .N), by = mainclass_id][order(-count)][1]\n\n\n\n  \n\n\n  # remove NA value on mainclass_id\n  combined3_1_data <- combined3_data[!is.na(mainclass_id)]\n  # find the top 10 companies type = 3 (worldwide) with most patent\n  top10_company <- combined3_1_data[type == 3, .(count = .N), by = organization][order(-count)][1:10][, count := NULL]\n  # subset only the top 10 companies\n  combined3_2_data <- setDT(combined3_1_data, key='organization')[J(top10_company)]\n  \n# 4.4.3.2 find top 5 mainclass_id from these top 10 companies\ncombined3_2_data[, .(count = .N), by = mainclass_id][order(-count)][1:5]"
  },
  {
    "objectID": "content/01_journal/04_data_visualization.html",
    "href": "content/01_journal/04_data_visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "In this section, the important process of data visualization is acquired in detail. Therefore, data can further be visualized for the importance of exploratory data analysis. Moreover, the concept of ggplot2 for building different data graphics and craft effective visualization for big data is introduced."
  },
  {
    "objectID": "content/01_journal/04_data_visualization.html#source-code-result",
    "href": "content/01_journal/04_data_visualization.html#source-code-result",
    "title": "Data Visualization",
    "section": "\n2.1 Source Code & Result",
    "text": "2.1 Source Code & Result\n\n# 1.0 LOAD LIBRARY ----\n\nlibrary(tidyverse)\nlibrary(ggrepel)\n\n# 2.0 DATA IMPORT ----\n\ncovid_data_tbl <- read_csv(\"https://covid.ourworldindata.org/data/owid-covid-data.csv\")\n\n#> Rows: 307912 Columns: 67\n#> ── Column specification ────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr   (4): iso_code, continent, location, tests_units\n#> dbl  (62): total_cases, new_cases, new_cases_smoothed, total_deaths, new_dea...\n#> date  (1): date\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# 3.0 DATA WRANGLING ----\n\n# 3.1 Manipulate for continent = Europe\n\n# Select columns and filter categories\ntotal_case_continent_tbl <- covid_data_tbl %>%\n  \n  select(continent, location, date, total_cases) %>%\n  \n  # Filter only Europe\n  filter(continent == \"Europe\" & date < Sys.Date()-4) %>%\n  \n  # Group by category and summarize\n  group_by(continent,location,date) %>%\n  summarise(total_cases = sum(total_cases)) %>%\n  ungroup() %>%\n  \n  # Replace all NA value to 0\n  replace(is.na(.), 0) %>%\n  \n  # Group by category and summarize\n  group_by(continent,date) %>%\n  summarise(total_cases = sum(total_cases)) %>%\n  ungroup() %>%\n  \n  # Rename column name\n  rename('Continent/ Country' = continent,\n         'Cumulative' = total_cases)\n\n#> `summarise()` has grouped output by 'continent', 'location'. You can override\n#> using the `.groups` argument.\n#> `summarise()` has grouped output by 'continent'. You can override using the\n#> `.groups` argument.\n\n# 3.2 Manipulate for location = Germany, UK, France, Spain, US\n\n# Select columns and filter categories\ntotal_case_location_tbl <- covid_data_tbl %>%\n  \n  select(location, date, total_cases) %>%\n  filter(location %in% c(\"Germany\",\"United Kingdom\",\"France\",\"Spain\",\"United States\") & date < Sys.Date()-4) %>%\n  \n  # Group by category and summarize\n  group_by(location, date) %>%\n  summarise(total_cases = sum(total_cases)) %>%\n  ungroup() %>%\n  \n  # Replace all NA value to 0\n  replace(is.na(.), 0) %>%\n  \n  # Rename column name\n  rename('Continent/ Country' = location,\n         'Cumulative' = total_cases)\n\n#> `summarise()` has grouped output by 'location'. You can override using the\n#> `.groups` argument.\n\n# 3.3 Join row from 2 tables\n\ntotal_case_combined <- total_case_continent_tbl %>%\n  bind_rows(total_case_location_tbl)\n\n# 3.4 Create Date 4 days prior the Sys.Date due to data is not completed at Sys.Date\n\nn = Sys.Date()-4\n\n\n# 3.5 Filter the last values to show labels on the plot\ndata_ends <- total_case_combined %>% \n  group_by(`Continent/ Country`) %>% \n  top_n(1, date) \n\n\n# 4.0 DATA VISUALIZATION ----\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(ggrepel)\n# Line Plot\ntotal_case_combined %>%\n  \n  ggplot(aes(x=date, y=`Cumulative`, group = `Continent/ Country`, color = `Continent/ Country`)) +\n  \n  geom_line(size = 1) +\n\n  scale_x_date(date_breaks=\"1 month\", date_labels = \"%B'%y\", limits = as.Date(c('2020-01-01','2023-06-01')) ) +\n\n  scale_y_continuous(labels = scales::number_format(scale = 1e-6, \n                                                    prefix = \"\",\n                                                    suffix = \" M\")) +\n  \n  scale_color_brewer(palette=\"Accent\") +\n  \n  labs(\n    title    = \"COVID-19 Confirmed Cases Worldwide\",\n    subtitle = str_glue(\"As of {n}\"),\n    x = \"\", # Override defaults for x and y\n    y = \"Cumulative Cases\"\n  ) + \n  \ntheme(\n  axis.text.x = element_text(angle = 45, hjust = 1),\n  legend.position = \"bottom\"\n) +\n\ngeom_label_repel(aes(label = scales::number(Cumulative)), \n                data = data_ends,\n                nudge_x = 1,\n                box.padding = 0.5,\n                show.legend = FALSE) \n\n#> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#> ℹ Please use `linewidth` instead."
  },
  {
    "objectID": "content/01_journal/04_data_visualization.html#source-code-result-1",
    "href": "content/01_journal/04_data_visualization.html#source-code-result-1",
    "title": "Data Visualization",
    "section": "\n3.1 Source Code & Result",
    "text": "3.1 Source Code & Result\n\n# 1.0 LOAD LIBRARY ----\n\nlibrary(tidyverse)\nlibrary(maps)\nlibrary(scales)\n\n\n# 2.0 DATA IMPORT ----\n\n# 2.1 for world map\nworld <- map_data(\"world\")\n# 2.2 for covid data\ncovid_data_tbl <- read_csv(\"https://covid.ourworldindata.org/data/owid-covid-data.csv\")\n\n#> Rows: 307912 Columns: 67\n#> ── Column specification ────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr   (4): iso_code, continent, location, tests_units\n#> dbl  (62): total_cases, new_cases, new_cases_smoothed, total_deaths, new_dea...\n#> date  (1): date\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# 3.0 DATA WRANGLING ----\n\n# 3.1 Revise the location name to match with the world map data\n\ncovid_data_tbl <- covid_data_tbl %>% \n  mutate(location = case_when(\n    \n    location == \"United Kingdom\" ~ \"UK\",\n    location == \"United States\" ~ \"USA\",\n    location == \"Democratic Republic of Congo\" ~ \"Democratic Republic of the Congo\",\n    location == \"Antigua and Barbuda\" ~ \"Antigua\",\n    location == \"Cote d'Ivoire\" ~ \"Ivory Coast\",\n    location == \"Czechia\" ~ \"Czech Republic\",\n    location == \"Timor\" ~ \"Timor-Leste\",\n    location == \"Trinidad and Tobago\" ~ \"Trinidad\",\n    TRUE ~ location\n    \n  )) \n\n\n# 3.2 Select columns and filter categories\nmortality_rate_tbl <- covid_data_tbl %>%\n  \n  select(location, date, total_deaths, population) %>%\n  \n  # Replace all NA values to 0\n  replace(is.na(.), 0) %>%\n\n  # Create mortality rate column\n  group_by(location) %>%\n  mutate(mortality_rate = total_deaths / population) %>%\n  ungroup() %>%\n\n  # Filter only certain date\n  filter(date == \"2022-04-16\") %>%\n  \n  # Rename column name\n  rename(region = location)\n\n# 3.3 Format as numeric value for mortality rate\nmortality_rate_tbl$mortality_rate <- as.numeric(mortality_rate_tbl$mortality_rate)\n\n# 3.4 Left join from 2 tables\n\nmortality_rate_map <- left_join(mortality_rate_tbl,world, by = \"region\")\n\n\n# 4.0 DATA VISUALIZATION ----\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(scales)\n# Map Plot\n\n  ggplot(mortality_rate_map, aes(map_id = region, fill = mortality_rate))+\n  geom_map(map = mortality_rate_map,  color = \"white\")+\n  expand_limits(x = mortality_rate_map$long, y = mortality_rate_map$lat) +\n  scale_fill_gradient(low='red', high='grey20', labels = percent) +\n\n  # add labels\n  labs(\n    title = \"Confirmed COVID-19 Deaths Relative to Population\",\n    caption = \"2022-04-16\",\n    x = \"\",\n    y = \"\",\n    ) +\n    \n  theme(\n    legend.position = \"right\"\n    )"
  },
  {
    "objectID": "content/01_journal/01_tidyverse.html",
    "href": "content/01_journal/01_tidyverse.html",
    "title": "Tidyverse",
    "section": "",
    "text": "In this section, process of importing data to visualizing data is acquired. The core concepts in the packages dplyr and ggplot2, which are parts of the tidyverse are introduced. These packages work together as part of larger data analysis pipeline."
  },
  {
    "objectID": "content/01_journal/01_tidyverse.html#challenge-1",
    "href": "content/01_journal/01_tidyverse.html#challenge-1",
    "title": "Tidyverse",
    "section": "\n2.1 Challenge 1",
    "text": "2.1 Challenge 1\nAnalyze the sales by location (state) with a bar plot. Which state has the highest revenue?"
  },
  {
    "objectID": "content/01_journal/01_tidyverse.html#challenge-2",
    "href": "content/01_journal/01_tidyverse.html#challenge-2",
    "title": "Tidyverse",
    "section": "\n2.2 Challenge 2",
    "text": "2.2 Challenge 2\nAnalyze the sales by location and year."
  },
  {
    "objectID": "content/01_journal/01_tidyverse.html#sales-by-year",
    "href": "content/01_journal/01_tidyverse.html#sales-by-year",
    "title": "Tidyverse",
    "section": "\n4.1 Sales by Year",
    "text": "4.1 Sales by Year\n\n# 6.0 Business Insights ----\n# 6.1 Sales by Year ----\nlibrary(lubridate)\nlibrary(dplyr)\n\n# Step 1 - Manipulate\n\nsales_by_year_tbl <- bike_orderlines_wrangled_tbl %>%\n  \n  # Select columns\n  select(order_date, total_price) %>%\n  # Add year column\n  mutate(year = year(order_date)) %>%\n  # Grouping by year and summarizing sales\n  group_by(year) %>% \n  summarize(sales = sum(total_price)) %>%\n  \n  # Optional: Add a column that turns the numbers into a currency format \n  # mutate(sales_text = scales::dollar(sales)) <- Works for dollar values\n  mutate(sales_text = scales::dollar(sales, big.mark = \".\", \n                                     decimal.mark = \",\", \n                                     prefix = \"\", \n                                     suffix = \" €\"))\n\nsales_by_year_tbl  \n\n\n\n  \n\n\n# Step 2 - Visualize\n\nlibrary(ggplot2)\nsales_by_year_tbl %>%\n  \n  # Setup canvas with the columns year (x-axis) and sales (y-axis)\n  ggplot(aes(x = year, y = sales)) +\n  \n  # Geometries\n  geom_col(fill = \"#2DC6D6\") + # Use geom_col for a bar plot\n  geom_label(aes(label = sales_text)) + # Adding labels to the bars\n  geom_smooth(method = \"lm\", se = FALSE) + # Adding a trendline\n\n  # Formatting\n  # scale_y_continuous(labels = scales::dollar) + # Change the y-axis. \n  # Adjust to euro values\n  scale_y_continuous(labels = scales::dollar_format(big.mark = \".\", \n                                                    decimal.mark = \",\", \n                                                    prefix = \"\", \n                                                    suffix = \" €\")) +\n  labs(\n    title    = \"Revenue by year\",\n    subtitle = \"Upward Trend\",\n    x = \"\", # Override defaults for x and y\n    y = \"Revenue\"\n  )\n\n#> `geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "content/01_journal/01_tidyverse.html#sales-by-year-and-category",
    "href": "content/01_journal/01_tidyverse.html#sales-by-year-and-category",
    "title": "Tidyverse",
    "section": "\n4.2 Sales by Year and Category",
    "text": "4.2 Sales by Year and Category\n\n# 6.2 Sales by Year and Category ----\nlibrary(lubridate)\nlibrary(dplyr)\nlibrary(ggplot2)\n# Step 1 - Manipulate\nsales_by_year_cat_1_tbl <- bike_orderlines_wrangled_tbl %>%\n\n  # Select columns and add a year\n  select(order_date, total_price, category_1) %>%\n  mutate(year = year(order_date)) %>%\n  \n  # Group by and summarize year and main category\n  group_by(year, category_1) %>%\n  summarise(sales = sum(total_price)) %>%\n  ungroup() %>%\n  \n  # Format $ Text\n  mutate(sales_text = scales::dollar(sales, big.mark = \".\", \n                                     decimal.mark = \",\", \n                                     prefix = \"\", \n                                     suffix = \" €\"))\n\n#> `summarise()` has grouped output by 'year'. You can override using the\n#> `.groups` argument.\n\nsales_by_year_cat_1_tbl  \n\n\n\n  \n\n\n# Step 2 - Visualize\nsales_by_year_cat_1_tbl %>%\n  \n  # Set up x, y, fill\n  ggplot(aes(x = year, y = sales, fill = category_1)) +\n  \n  # Geometries\n  geom_col() + # Run up to here to get a stacked bar plot\n  geom_smooth(method = \"lm\", se = FALSE) + # Adding a trendline\n  \n  # Facet\n  facet_wrap(~ category_1) +\n  \n  # Formatting\n  scale_y_continuous(labels = scales::dollar_format(big.mark = \".\", \n                                                    decimal.mark = \",\", \n                                                    prefix = \"\", \n                                                    suffix = \" €\")) +\n  labs(\n    title = \"Revenue by year and main category\",\n    subtitle = \"Each product category has an upward trend\",\n    fill = \"Main category\" # Changes the legend name\n  )\n\n#> `geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "content/01_journal/01_tidyverse.html#sales-by-location",
    "href": "content/01_journal/01_tidyverse.html#sales-by-location",
    "title": "Tidyverse",
    "section": "\n4.3 Sales by Location",
    "text": "4.3 Sales by Location\n\n# 6.3 Sales by Location ----\nlibrary(lubridate)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Step 1 - Manipulate\nsales_by_loc_tbl <- bike_orderlines_wrangled_tbl %>%\n  \n  # Select columns and add a year\n  select(order_date, total_price, state) %>%\n  mutate(year = year(order_date)) %>%\n  \n  # Group by and summarize year and main catgegory\n  group_by(state) %>%\n  summarise(sales = sum(total_price)) %>%\n  #ungroup() %>%\n  \n  # Format $ Text\n  mutate(sales_text = scales::dollar(sales, big.mark = \".\", \n                                     decimal.mark = \",\", \n                                     prefix = \"\", \n                                     suffix = \" €\"))\n\nsales_by_loc_tbl\n\n\n\n  \n\n\n\n\n# Step 2 - Visualize\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(lubridate)\n\nsales_by_loc_tbl %>%\n  \n  # Setup canvas with the columns year (x-axis) and sales (y-axis)\n  ggplot(aes(x = state, y = sales)) +\n  \n  # Geometries\n  geom_col(fill = \"#2DC6D6\") + # Use geom_col for a bar plot\n  geom_label(aes(label = sales_text)) + # Adding labels to the bars\n  geom_smooth(method = \"lm\", se = FALSE) + # Adding a trendline\n  \n  # Formatting\n  # scale_y_continuous(labels = scales::dollar) + # Change the y-axis. \n  # Adjust to euro values\n  scale_y_continuous(labels = scales::dollar_format(big.mark = \".\", \n                                                    decimal.mark = \",\", \n                                                    prefix = \"\", \n                                                    suffix = \" €\")) +\n  labs(\n    title    = \"Revenue by year\",\n    x = \"\", # Override defaults for x and y\n    y = \"Revenue\"\n  ) + \n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n#> `geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "content/01_journal/01_tidyverse.html#sales-by-year-and-location",
    "href": "content/01_journal/01_tidyverse.html#sales-by-year-and-location",
    "title": "Tidyverse",
    "section": "\n4.4 Sales by Year and Location",
    "text": "4.4 Sales by Year and Location\n\n# 6.4 Sales by Year and Location ----\nlibrary(dplyr)\n\n# Step 1 - Manipulate\nsales_by_year_loc_tbl <- bike_orderlines_wrangled_tbl %>%\n  \n  # Select columns and add a year\n  select(order_date, total_price, state) %>%\n  mutate(year = year(order_date)) %>%\n  \n  # Group by and summarize year and main category\n  group_by(year, state) %>%\n  summarise(sales = sum(total_price)) %>%\n  ungroup() %>%\n  \n  # Format $ Text\n  mutate(sales_text = scales::dollar(sales, big.mark = \".\", \n                                     decimal.mark = \",\", \n                                     prefix = \"\", \n                                     suffix = \" €\"))\n\n#> `summarise()` has grouped output by 'year'. You can override using the\n#> `.groups` argument.\n\nsales_by_year_loc_tbl  \n\n\n\n  \n\n\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lubridate)\n# Step 2 - Visualize\nsales_by_year_loc_tbl %>%\n  \n  # Set up x, y, fill\n  ggplot(aes(x = year, y = sales, fill = state)) +\n  \n  # Geometries\n  geom_col() + # Run up to here to get a stacked bar plot\n  geom_smooth(method = \"lm\", se = FALSE) + # Adding a trendline\n  \n  # Facet\n  facet_wrap(~ state) +\n  \n  # Formatting\n  scale_y_continuous(labels = scales::dollar_format(big.mark = \".\", \n                                                    decimal.mark = \",\", \n                                                    prefix = \"\", \n                                                    suffix = \" €\")) +\n  labs(\n    title = \"Revenue by year and state\",\n    #subtitle = \"Each product category has an upward trend\",\n    fill = \"State\" # Changes the legend name\n  ) +\ntheme(\n  axis.text.x = element_text(angle = 45, hjust = 1)\n)\n\n#> `geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "content/01_journal/01_tidyverse.html#writing-files",
    "href": "content/01_journal/01_tidyverse.html#writing-files",
    "title": "Tidyverse",
    "section": "\n4.5 Writing Files",
    "text": "4.5 Writing Files\n\n# 7.0 Writing Files ----\nlibrary(tidyverse)\n\n# 7.1 Excel ----\n\nlibrary(\"writexl\")\nbike_orderlines_wrangled_tbl %>%\n  write_xlsx(\"../../00_data/01_bike_sales/02_wrangled_data/bike_orderlines.xlsx\")\n\n# 7.2 CSV ----\nbike_orderlines_wrangled_tbl %>% \n  write_csv(\"../../00_data/01_bike_sales/02_wrangled_data/bike_orderlines.csv\")\n\n# 7.3 RDS ----\nbike_orderlines_wrangled_tbl %>% \n  write_rds(\"../../00_data/01_bike_sales/02_wrangled_data/bike_orderlines.rds\")"
  },
  {
    "objectID": "content/01_journal/02_data_acquisition.html",
    "href": "content/01_journal/02_data_acquisition.html",
    "title": "Data Acquisition",
    "section": "",
    "text": "In this section, process of importing data or data acquisition is acquired in detail. Therefore, imported data can further be viewed, stored, and analyzed. Web scraping process is introduced to gather available data on the internet. Connecting to database and acquiring the data from web will be presented."
  },
  {
    "objectID": "content/01_journal/02_data_acquisition.html#source-code-result",
    "href": "content/01_journal/02_data_acquisition.html#source-code-result",
    "title": "Data Acquisition",
    "section": "\n2.1 Source Code & Result",
    "text": "2.1 Source Code & Result\n\n# Load library\nlibrary(httr)\nresp <- GET(\"https://api.open-meteo.com/v1/forecast?latitude=53.55&longitude=9.99&hourly=temperature_2m&timeformat=unixtime\")\n\nlibrary(jsonlite)\nlibrary(dplyr)\n\n#> \n#> Attaching package: 'dplyr'\n\n\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n\n\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\n\nresp_tbl <- \n  resp$content %>% \n  rawToChar() %>% \n  fromJSON() %>%\n  .[[9]] %>%\n  as_tibble()\n\n\nresp_tbl$time <- as.POSIXct(resp_tbl$time, origin=\"1970-01-01\")\n\n\n# Step 2 - Visualize\nlibrary(ggplot2)\n  \n  # Setup canvas with the columns year (x-axis) and sales (y-axis)\n  ggplot(data=resp_tbl, aes(x = time, y = temperature_2m, group=1)) +\n  \n  # Geometries\n  geom_line(color=\"cyan\") + # Use geom_line(color) for a color line\n\n  # Formatting\n\n  labs(\n    title    = \"Weather forecast in Hamburg\",\n    subtitle = \"for 7 days\",\n    x = \"\", # Override defaults for x and y\n    y = \"Temperature 2m\"\n    ) + \n    theme(\n      axis.text.x = element_text(angle = 90, hjust = 1)\n    )"
  },
  {
    "objectID": "content/01_journal/02_data_acquisition.html#source-code-result-1",
    "href": "content/01_journal/02_data_acquisition.html#source-code-result-1",
    "title": "Data Acquisition",
    "section": "\n3.1 Source Code & Result",
    "text": "3.1 Source Code & Result\n\n# WEBSCRAPING ----\n\n# 1.0 LIBRARIES ----\n\nlibrary(tidyverse) # Main Package - Loads dplyr, purrr, etc.\nlibrary(rvest)     # HTML Hacking & Web Scraping\nlibrary(xopen)     # Quickly opening URLs\nlibrary(jsonlite)  # converts JSON files to R objects\nlibrary(glue)      # concatenate strings\nlibrary(stringi)   # character string/text processing\n\n# 1.1 ROSE HOME PRODUCTS FAMILIES ----\n\nurl_home          <- \"https://www.rosebikes.com/\"\nxopen(url_home) # Open links directly from RStudio to inspect them\n\n#> Running open 'https://www.rosebikes.com/'\n\n# Read in the HTML for the entire webpage\nhtml_home         <- read_html(url_home)\n\n# Web scrape the ids for the families\nrose_home_tbl <- html_home %>%\n  \n  # Get the nodes for the families ...\n  html_nodes(css = \".js-main-navigation__item-button\") %>%\n  # ...and extract the information of the id attribute\n  html_attr('href')  %>%\n  \n  # Remove the product families Gear and Outlet and Woman \n  # (because the female bikes are also listed with the others)\n  discard(.p = ~stringr::str_detect(.x,\"clothing|parts|accessories|sale|brands\")) %>%\n  \n  # Convert vector to tibble\n  enframe(name = \"position\", value = \"home_class\") %>%\n  \n  # Add a hashtag so we can get nodes of the categories by id (#)\n  mutate(\n    url = glue(\"https://www.rosebikes.com/{home_class}\")\n  )\n\nurl_bikes <- rose_home_tbl$url\nurl_bikes\n\n#> https://www.rosebikes.com//bikes\n\n# 1.1 COLLECT PRODUCT FAMILIES ----\n\n\nxopen(url_bikes) # Open links directly from RStudio to inspect them\n\n#> Running open 'https://www.rosebikes.com//bikes'\n\n# Read in the HTML for the entire webpage\nhtml_bikes         <- read_html(url_bikes)\n\n# Web scrape the ids for the families\nbike_family_tbl <- html_bikes %>%\n  \n  # Get the nodes for the families ...\n  html_nodes(css = \".catalog-navigation__link\") %>%\n  # ...and extract the information of the id attribute\n  html_attr('href')  %>%\n\n  # Remove the product families Gear and Outlet and Woman \n  # (because the female bikes are also listed with the others)\n  discard(.p = ~stringr::str_detect(.x,\"urban|short-delivery\")) %>%\n  \n  # Convert vector to tibble\n  enframe(name = \"position\", value = \"family_class\") %>%\n  \n  # Add a hashtag so we can get nodes of the categories by id (#)\n  mutate(\n    url = glue(\"https://www.rosebikes.com/{family_class}\")\n  )\n\n\n# 2.0 COLLECT BIKE DATA ----\n\n# 2.1 Get URL for each bike of the Product categories\n\n# select first bike category url\nbike_category_url <- bike_family_tbl$url[1]\nxopen(bike_category_url)\n\n#> Running open 'https://www.rosebikes.com//bikes/mtb'\n\n# Get the URLs for the bikes of the first category\nhtml_bike_category  <- read_html(bike_category_url)\n\nbike_url_tbl        <- html_bike_category %>%\n\n  # Get the 'a' nodes, which are hierarchally underneath \n  # the class productTile__contentWrapper\n  html_nodes(css = \".catalog-category-bikes__button\") %>%\n  html_attr(\"href\") %>%\n  \n  # Convert vector to tibble\n  enframe(name = \"position\", value = \"bike_model_class\") %>%\n  \n  # Add a hashtag so we can get nodes of the categories by id (#)\n  mutate(\n    url = glue(\"https://www.rosebikes.com/{bike_model_class}\")\n  )\n\n\n# 2.1.2 Extract the price (since we have retrieved the data already)\nbike_price_tbl <- html_bike_category %>%\n  \n  # Get the nodes in the meta tag where the attribute itemprop equals description\n  html_nodes(\".catalog-category-bikes__price-title\") %>%\n  \n  # Extract the content of the attribute content\n  html_text() %>%\n\n  # Convert vector to tibble\n  enframe(name = \"position\", value = \"price\") \n\nbike_price_tbl$price <- gsub('[from €,]','',(bike_price_tbl$price))\nbike_price_tbl$price <- as.numeric(bike_price_tbl$price)\n\n# 2.2 Wrap it into a function ----\n\nget_bike_data <- function(url) {\n  \n  html_bike_category <- read_html(url)\n  \n  # Get the URLs\n  bike_url_tbl  <- html_bike_category %>%\n    html_nodes(css = \".catalog-category-bikes__button\") %>%\n    html_attr(\"href\") %>%\n    enframe(name = \"position\", value = \"bike_model_class\") %>%\n    mutate(\n      url = glue(\"https://www.rosebikes.com/{bike_model_class}\")\n    )\n  # Get the price\n  bike_price_tbl <- html_bike_category %>%\n    html_nodes(\".catalog-category-bikes__price-title\") %>%\n    html_text() %>%\n    enframe(name = \"position\", value = \"price_in_Euro\") %>%\n    left_join(bike_url_tbl)\n}\n\nbike_category_url <- bike_family_tbl$url[1]\nbike_data_tbl     <- get_bike_data(url = bike_category_url)\n\n#> Joining with `by = join_by(position)`\n\nbike_data_tbl$bike_model_class <- gsub('-/-','-',(bike_data_tbl$bike_model_class))\n\n# 2.3.1a Map the function against all urls\n\n# Extract the urls as a character vector\nbike_category_url_vec <- bike_family_tbl %>% \n  pull(url)\n\n# Run the function with every url as an argument\nbike_data_lst <- map(bike_category_url_vec, get_bike_data)\n\n#> Joining with `by = join_by(position)`\n#> Joining with `by = join_by(position)`\n#> Joining with `by = join_by(position)`\n#> Joining with `by = join_by(position)`\n#> Joining with `by = join_by(position)`\n#> Joining with `by = join_by(position)`\n\n# Merge the list into a tibble\nbike_data_tbl <- bind_rows(bike_data_lst)\nsaveRDS(bike_data_tbl, \"bike_data_tbl.rds\")\n\nbike_data_tbl$bike_model_class <- gsub('-/-','-',(bike_data_tbl$bike_model_class))\n\n# Filter non Canyon bikes (based on id length) and add an empty column for the colors\nbike_data_cleaned_tbl <- bike_data_tbl %>%\n  \n  # Filter for bikes. Only unique ones\n  \n  # Split categories ()\n  separate(col = bike_model_class,\n           into = c(\"empty\",\n                    \"product\",\n                    \"model\",\n                    \"ride_type\",\n                    \"ride_style\"),\n           sep = \"/\") %>%\n\n  # Select and order columns\n\n  select(product, model, ride_type, ride_style, url, price_in_Euro)\n\n#> Warning: Expected 5 pieces. Missing pieces filled with `NA` in 9 rows [12, 13, 14, 15,\n#> 16, 17, 18, 19, 20].\n\nbike_data_cleaned_tbl$price_in_Euro <- gsub('[from €,]','',(bike_data_cleaned_tbl$price_in_Euro))\nbike_data_cleaned_tbl$price_in_Euro <- as.numeric(bike_data_cleaned_tbl$price_in_Euro)\n\n\nbike_data_cleaned_tbl\n\n\n\n  \n\n\n# Save data as RDS\n\nsaveRDS(bike_data_cleaned_tbl, \"bike_data_cleaned_tbl.rds\")"
  },
  {
    "objectID": "content/02_notes/05_class_notes.html",
    "href": "content/02_notes/05_class_notes.html",
    "title": "Class Notes",
    "section": "",
    "text": "IMPORTANT: You can delete everything in here and start fresh. You might want to start by not deleting anything above this line until you know what that stuff is doing.\nThis is an .qmd file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a # in front of your text, it will create a top level-header."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "Business Data Science Basics lab journaling for data science courses at the Institute of Entrepreneurship, which is part of Business & Management class."
  },
  {
    "objectID": "index.html#student-data",
    "href": "index.html#student-data",
    "title": "My Lab Journal",
    "section": "Student data:",
    "text": "Student data:\n\nName: Muhammad Adrian\nMatriculation Number: 537524\nProgram: Chemical and Bioprocess Engineering"
  }
]